{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple Chatbot from Scratch in Python (using NLTK)\n",
    "\n",
    "\n",
    "References:\n",
    "https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. \n",
    "\n",
    "By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and installing NLTK\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing NLTK Packages\n",
    "\n",
    "##### WordNetLemmatizer\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n",
    "\n",
    "https://www.geeksforgeeks.org/python-lemmatization-with-nltk/\n",
    "\n",
    "##### Punkt Sentence Tokenizer\n",
    "\n",
    "This tokenizer divides a text into a list of sentences\n",
    "by using an unsupervised algorithm to build a model for abbreviation\n",
    "words, collocations, and words that start sentences.  It must be\n",
    "trained on a large collection of plaintext in the target language\n",
    "before it can be used.\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "\n",
    "#####  Wordnet\n",
    "\n",
    "WordNet is a lexical database for the English language, which was created by Princeton, and is part of the NLTK corpus.\n",
    "\n",
    "You can use WordNet alongside the NLTK module to find the meanings of words, synonyms, antonyms, and more. Let's cover some examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True) # for downloading packages\n",
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the corpus\n",
    "\n",
    "For our example,we will be using the Wikipedia page for chatbots and KJSCE Information as our corpus. Copy the contents from the page and place it in a text file named ‘chatbot.txt’. However, you can use any corpus of your choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('chatbot.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw = raw.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_tokens : \n",
      " ['a chatbot (also known as a talkbot, chatterbot, bot, im bot, interactive agent, or artificial conversational entity) is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.', 'such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the turing test.', 'chatbots are typically used in dialog systems for various practical purposes including customer service or information acquisition.', 'some chatterbots use sophisticated natural language processing systems, but many simpler systems scan for keywords within the input, then pull a reply with the most matching keywords, or the most similar wording pattern, from a database.', 'the term \"chatterbot\" was originally coined by michael mauldin (creator of the first verbot, julia) in 1994 to describe these conversational programs.today, most chatbots are either accessed via virtual assistants such as google assistant and amazon alexa, via messaging apps such as facebook messenger or wechat, or via individual organizations\\' apps and websites.']\n",
      "\n",
      "\n",
      "word_tokens : \n",
      " ['a', 'chatbot', '(', 'also', 'known']\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
    "\n",
    "#Example how tokenisation work\n",
    "print(\"sent_tokens : \\n\", sent_tokens[0:5])\n",
    "print('\\n')\n",
    "print(\"word_tokens : \\n\", word_tokens[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We shall now define a function called LemTokens which will take as input the tokens and return normalized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword matching\n",
    "\n",
    "Next, we shall define a function for a greeting by the bot i.e if a user’s input is a greeting, the bot shall return a greeting response.ELIZA uses a simple keyword matching for greetings. We will utilize the same concept here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Response\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "if our dictionary contains the words {Learning, is, the, not, great}, and we want to vectorize the text “Learning is great”, we would have the following vector: (1, 1, 0, 0, 1).\n",
    "\n",
    "https://www.geeksforgeeks.org/bag-of-words-bow-model-in-nlp/\n",
    "\n",
    "### TF-IDF Approach\n",
    "\n",
    "**Term Frequency: is a scoring of the frequency of the word in the current document.**\n",
    "\n",
    "```\n",
    "TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "```\n",
    "https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76\n",
    "\n",
    "**Inverse Document Frequency: is a scoring of how rare the word is across documents.**\n",
    "\n",
    "```\n",
    "IDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "```\n",
    "### Cosine Similarity\n",
    "\n",
    "Tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus\n",
    "\n",
    "```\n",
    "Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||\n",
    "```\n",
    "where d1,d2 are two non zero vectors.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a response from our bot for input questions, the concept of document similarity will be used. We define a function response which searches the user’s utterance for one or more known keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response:” I am sorry! I don’t understand you”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will feed the lines that we want our bot to say while starting and ending a conversation depending upon user’s input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots and KJSCE. If you want to exit, type Bye!\n",
      "You: Hey\n",
      "ROBO: I am glad! You are talking to me\n",
      "You: whats the time\n",
      "2020-08-05 18:41:02.883474\n",
      "You: tell me about chatbot design \n",
      "ROBO: design\n",
      "the chatbot design is the process that defines the interaction between the user and the chatbot.the chatbot designer will define the chatbot personality, the questions that will be asked to the users, and the overall interaction.it can be viewed as a subset of the conversational design.\n",
      "You: can you tell me about kjsce\n",
      "ROBO: kjsce\n",
      "kjsce is one of the best engineering colleges in india, ranking in the top amongst b.tech and m.tech colleges for its excellent education.\n",
      "You: what are the internship opportunities?\n",
      "ROBO: internship cell\n",
      "internships are educational and career development opportunities, providing practical experience in a field or discipline.\n",
      "You: what is team onyx\n",
      "ROBO: team onyx india\n",
      "team onyx india, the aero design club of k j somaiya college of engineering, are a group of enthusiasts who have a passion for all things related to the world of aviation.\n",
      "You: resume\n",
      "ROBO: resume\n",
      "students should submit the resume as per the template provided by the college.\n",
      "You: ninad\n",
      "ROBO: ninad mehendale\n",
      "(2020).facial emotion recognition using convolutional neural networks (ferc).springer nature applied sciences,3(2):1-8.https://doi.org/10.1007/s42452-020-2234-1.\n",
      "You: thank you\n",
      "ROBO: You are welcome..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots and KJSCE. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input('You: ')\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        elif 'time' in user_response:\n",
    "            strTime = datetime.datetime.now()\n",
    "            response(user_response)\n",
    "            print(strTime)\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "                \n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank You\n",
    "\n",
    "\n",
    "#### Alumni Cell KJSCE\n",
    "\n",
    "#### Rutuja Rane & Harshal Shah\n",
    "\n",
    "#### Jose Portilla Instructor on Udemy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
